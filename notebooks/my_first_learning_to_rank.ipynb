{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"my_first_ltr\"): # type: ignore\n",
    "    %pip install -qqq git+https://github.com/algolia/my-first-learning-to-rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Learning To Rank ?\n",
    "\n",
    "Ranking is the process of organizing or arranging items in order based on their importance, quality, or performance. It is widely used in various applications, such as search engines, where results are ranked to show the most relevant pages first, or in recommender systems, which rank products, movies, or other items to suggest the most suitable options to users.\n",
    "\n",
    "## Why rank ? And what's ranking here ?\n",
    "In the context of a search experience ranking helps prioritize the most relevant results, improving the user experience by making it easier to find what they need quickly. We will try to define an optimal order for the results for each query. What we define by query here is the word or phrase a user types to find information.\n",
    "\n",
    "## What's ranking model ?\n",
    "\n",
    "It's a function that maps an item to it's relevance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It all starts with data\n",
    "\n",
    "- a user search history on a streaming platform\n",
    "- a subset of imdb dataset\n",
    "\n",
    "At home, you can try with your search history if you'd like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all the nasty pre-processing and cleaning for you - so you can just have fun! \n",
    "The main steps we did for preprocessing are:\n",
    "- One-hot encoding: turn multi-categorical features into a list of binary features\n",
    "- Create a textual relevance signal (using [OkapiBM25](https://en.wikipedia.org/wiki/Okapi_BM25))\n",
    "- Compute the relevance score of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are our features ?\n",
    "What is a feature in our context? A feature in machine learning is a piece of information or characteristic that helps the model make predictions or decisions. Itâ€™s an input the model uses to learn patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.utils import load_raw_dataset\n",
    "\n",
    "unprocess_dataset = load_raw_dataset()\n",
    "unprocess_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead! Explore the dataset to get familiar with it a bit, here are a few examples for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram for numerical values:\n",
    "unprocess_dataset.imdb_score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram for textual values (note the use of explode when it's a list of string):\n",
    "unprocess_dataset.explode(\"genres\").genres.value_counts().plot.barh()\n",
    "unprocess_dataset[unprocess_dataset.Action == \"play\"].explode(\"genres\").genres.value_counts().plot.barh(color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between values:\n",
    "unprocess_dataset[[\"imdb_score\", \"tmdb_score\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's our score here ?\n",
    "\n",
    "- What's your idea ?\n",
    "\n",
    "We base our scoring on the past interactions of the users with the movies when they typed a query. \n",
    "- We consider that if the user watched the movie, it was highly relevant\n",
    "- We consider that if the user added it to it's watchlist, it was relevant, but not the right mood at that time\n",
    "- We consider that if the user clicked on a movie, it showed some interest but it wasn't that relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.utils import load_dataset\n",
    "\n",
    "dataset = load_dataset(local=True)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also negative examples. It would help to improve the model by providing contrast to positive examples. Using a conversion ratio in the score, instead of a simple sum, helps the model better capture the relative importance of examples, leading to more balanced and accurate rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and testing sets to train the model on one portion of the data and evaluate its performance on unseen data, ensuring it generalizes well to new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.train_utils import get_categories\n",
    "import pandas as pd\n",
    "from catboost import Pool\n",
    "\n",
    "def dataset_split(\n",
    "    dataset: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the dataset into training, testing, and validation sets based on queries.\n",
    "\n",
    "    Steps to implement:\n",
    "    1. Group the dataset by the \"normalized_query\"\n",
    "    2. Split the grouped data into two sets: 95% for training and testing, 5% for validation.\n",
    "    3. Further split the 95% dataset into 70% for training and 30% for testing.\n",
    "    4. Explode the grouped data back into individual rows for each query.\n",
    "    5. Return the Datasets\n",
    "    \"\"\"\n",
    "    # FIXME: Complete the function here\n",
    "    pass\n",
    "\n",
    "def build_pool(dataset: pd.DataFrame, name: str) -> Pool:\n",
    "    \"\"\"\n",
    "    Given a dataset and its name, build a CatBoost Pool object.\n",
    "\n",
    "    Steps to implement:\n",
    "    1. Sort the dataset by the \"normalized_query\" column.\n",
    "    2. Prepare the input features:\n",
    "       - Exclude columns that are not input features (e.g., \"normalized_query\", \"id\", \"score\").\n",
    "    3. Extract:\n",
    "       - Features\n",
    "       - Target column (\"score\") as labels.\n",
    "    4. To identify the categorical features in the dataset ou can use the function `get_categories(dataset: pd.DataFrame)`.\n",
    "    5. Construct and return the `Pool` (Group identifiers from the \"normalized_query\" column with the parameter `group_id`).\n",
    "    \"\"\"\n",
    "    # FIXME: Complete the function here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.train_utils import build_pool, dataset_split\n",
    "\n",
    "\n",
    "train_df, test_df, val_df = dataset_split(dataset)\n",
    "\n",
    "train_pool = build_pool(train_df, \"train\")\n",
    "test_pool = build_pool(test_df, \"test\")\n",
    "val_pool = build_pool(val_df, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then comes a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise: RMSE\n",
    "\n",
    "A pointwise learning-to-rank (LTR) approach using Root Mean Square Error (RMSE) is a method where the ranking problem is treated as a regression problem. The model is trained to predict a relevance score as close as possible to the ground truth relevance score for each individual item.\n",
    "\n",
    "**It ignores the relationships between items within a list, focusing only on the accuracy of individual predictions. Thus it's name, pointwise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go to practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initializes a CatBoostRanker, a gradient-boosting model  using the following parameters:\n",
    "\n",
    "- `loss_function=\"RMSE\"`: Optimize based on `RMSE` which measures the average squared difference between predicted and true ranks.\n",
    "- `learning_rate=0.15`: Determines how much the model's parameters are updated in response to the calculated error after each iteration. A smaller value leads to slower, more stable learning, while a larger value speeds learning but risks overshooting the optimal solution.\n",
    "- `thread_count=1`: Uses a single CPU thread for training.\n",
    "- `iterations=500`: Runs 500 iterations of boosting (adding weak learners to improve predictions).\n",
    "- `random_seed=0`: Ensures reproducible results by fixing randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostRanker\n",
    "\n",
    "model = CatBoostRanker(loss_function=\"RMSE\", depth=6, learning_rate=0.15, thread_count=1, iterations=500, random_seed=0)\n",
    "\n",
    "model.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "val_df[\"pred_score\"] = model.predict(val_pool)\n",
    "\n",
    "results = val_df[[\"score\", \"pred_score\"]]\n",
    "\n",
    "print(\"Predictions vs Actuals:\")\n",
    "print(results.head())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(results[\"score\"], results[\"pred_score\"]))\n",
    "mae = mean_absolute_error(results[\"score\"], results[\"pred_score\"])\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise: PairLogit\n",
    "\n",
    "This approach focuses on learning the relative preference between pairs of items. The model is trained to predict which of the two items in a pair should be ranked higher, based on pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: setup the ranker for some pairwise ranking\n",
    "model_pairwise = CatBoostRanker(loss_function=\"PairLogit\", thread_count=1, random_seed=0)\n",
    "model_pairwise.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listwise: YetiRank\n",
    "\n",
    "YetiRank optimizes a smooth approximation of an IR (Information Retrieval) metric, such as NDCG (Normalized Discounted Cumulative Gain). The use of a listwise approach means that the model learns directly to improve the ranking quality of the entire list rather than individual scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, NDC what ?\n",
    "\n",
    "Normalized Discounted Cumulative Gain (NDCG) is a metric used to evaluate the quality of a ranked list of items. It measures how well the ranking of retrieved items matches the ideal ranking based on relevance, emphasizing the importance of placing highly relevant items near the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents ordered according to their relevance scores\n",
    "from numpy import asarray\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "true_relevance = asarray([list(reversed(range(21)))])\n",
    "print(true_relevance)\n",
    "print(\"Perfect:\", ndcg_score(true_relevance, true_relevance))\n",
    "\n",
    "pred_relevance = asarray([[19, 20] + list(reversed(range(19)))])\n",
    "print(pred_relevance)\n",
    "print(\"Two items swapping places:\", ndcg_score(true_relevance, pred_relevance))\n",
    "\n",
    "pred_relevance = asarray([[15, 19, 18, 17, 16, 20] + list(reversed(range(15)))])\n",
    "print(pred_relevance)\n",
    "print(\"Two items swapping places further down:\", ndcg_score(true_relevance, pred_relevance))\n",
    "\n",
    "pred_relevance = asarray([list(range(21))])\n",
    "print(pred_relevance)\n",
    "print(\"Let's reverse everything\", ndcg_score(true_relevance, pred_relevance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: setup the ranker for some listwise ranking\n",
    "# As you can see, we have to specify the end metric required to optimize here, (eg: CTR, CVR, NDCG) as we are not basing\n",
    "# the optimization of the scores difference to prediction.\n",
    "model_listwise = CatBoostRanker(loss_function=\"YetiRank\", thread_count=1, random_seed=0, custom_metric=[\"NDCG:top=-1;type=Base;denominator=LogPosition;hints=skip_train~false\"])\n",
    "model_listwise.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's leaderboard\n",
    "\n",
    "Create a model's leaderboard add add your iteration to compare to our baselines!\n",
    "\n",
    "Little tip to get you started, you can use catboost `get_eval` method to quickly retrieve a metric for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.data_visualisation import RMSE, NDCG_20\n",
    "\n",
    "# FIXME: try out the eval_metrics\n",
    "your_metric = ...\n",
    "model.eval_metrics(train_pool, your_metric, ntree_start=model.tree_count_ - 1)\n",
    "\n",
    "# FIXME: Compare the different models.\n",
    "models = {\"RMSE\": model, \"PairLogit\": model_pairwise, \"YetiRank\": model_listwise, \"BestModelInTheWorld\": ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution to retrieve metrics for multiple models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "models = {\"RMSE\": model, \"PairLogit\": model_pairwise, \"YetiRank\": model_listwise}\n",
    "metrics = []\n",
    "\n",
    "for k, m in models.items():\n",
    "    metrics_dict = dict()\n",
    "    metrics_dict[\"model_name\"] = k\n",
    "    metrics_dict['train_NDCG@20'] = m.eval_metrics(train_pool,\n",
    "                                                      'NDCG:top=20;type=Base;denominator=LogPosition',\n",
    "                                                      ntree_start=m.tree_count_ - 1)['NDCG:top=20;type=Base'][0]\n",
    "\n",
    "    metrics_dict['test_NDCG@20'] = m.eval_metrics(test_pool,\n",
    "                                                     'NDCG:top=20;type=Base;denominator=LogPosition',\n",
    "                                                     ntree_start=m.tree_count_ - 1)['NDCG:top=20;type=Base'][0]\n",
    "\n",
    "    metrics_dict['val_NDCG@20'] = m.eval_metrics(val_pool,\n",
    "                                                'NDCG:top=20;type=Base;denominator=LogPosition',\n",
    "                                                ntree_start=m.tree_count_ - 1)['NDCG:top=20;type=Base'][0]\n",
    "    metrics.append(metrics_dict)\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame.from_records(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoost, Pool\n",
    "from my_first_ltr.train_utils import keep_input_features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_dataset_shape_cascade( m: CatBoost, X: pd.DataFrame, pools: dict[str, Pool]) -> None:\n",
    "    \"\"\"Shap values average per feature accross all dataset.\"\"\"\n",
    "    df_feature_importance = pd.DataFrame(\n",
    "        data={k: m.get_feature_importance(pool) for k, pool in pools.items()}, index=X.columns\n",
    "    )\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    df_feature_importance.plot.barh(figsize=(10, 12))\n",
    "    plt.title(\"cross dataset - importance of each feature\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "cross_dataset_shape_cascade(model, keep_input_features(train_df), {\"train\": train_pool, \"test\": test_pool, \"val\": val_pool})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
