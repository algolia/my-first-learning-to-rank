{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0pdys6eKquE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import importlib\n",
        "\n",
        "if not importlib.util.find_spec(\"my_first_ltr\"): # type: ignore\n",
        "    %pip install -qqq git+https://github.com/algolia/my-first-learning-to-rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHcQpeAjKquG"
      },
      "source": [
        "# What's Learning To Rank ?\n",
        "\n",
        "Ranking is the process of organizing or arranging items in order based on their importance, quality, or performance. It is widely used in various applications, such as search engines, where results are ranked to show the most relevant pages first, or in recommender systems, which rank products, movies, or other items to suggest the most suitable options to users.\n",
        "\n",
        "## Why rank ? And what's ranking here ?\n",
        "In the context of a search experience ranking helps prioritize the most relevant results, improving the user experience by making it easier to find what they need quickly. We will try to define an optimal order for the results for each query. What we define by query here is the word or phrase a user types to find information.\n",
        "\n",
        "## What's ranking model ?\n",
        "\n",
        "A ranking model is a system that orders items based on how relevant or important they are, often giving each item a score. It takes input data, like features or preferences, and uses these scores to sort the items, placing the most relevant ones at the top."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TBtQ-e1KquH"
      },
      "source": [
        "# It all starts with data\n",
        "\n",
        "- a user search history on a streaming platform\n",
        "- a subset of imdb dataset\n",
        "\n",
        "At home, you can try with your search history if you'd like!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmiCWSKJKquH"
      },
      "source": [
        "We did all the nasty pre-processing and cleaning for you - so you can just have fun!\n",
        "The main steps we did for preprocessing are:\n",
        "- One-hot encoding: turn multi-categorical features into a list of binary features\n",
        "- Create a textual relevance signal (using [OkapiBM25](https://en.wikipedia.org/wiki/Okapi_BM25))\n",
        "- Compute the relevance score of the documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jms7DYwxKquI"
      },
      "source": [
        "## What are our features ?\n",
        "What is a feature in our context? A feature in machine learning is a piece of information or characteristic that helps the model make predictions or decisions. Itâ€™s an input the model uses to learn patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVgwAvakKquI"
      },
      "outputs": [],
      "source": [
        "from my_first_ltr.utils import load_raw_dataset\n",
        "\n",
        "unprocess_dataset = load_raw_dataset()\n",
        "unprocess_dataset.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpmbqM8kKquI"
      },
      "outputs": [],
      "source": [
        "# Go ahead! Explore the dataset to get familiar with it a bit, here are a few examples for you!\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLZJtlV8KquI"
      },
      "outputs": [],
      "source": [
        "# histogram for numerical values:\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "unprocess_dataset.imdb_score.hist()\n",
        "plt.title(\"IMDB score values repartition\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29HJ7oZUKquJ"
      },
      "outputs": [],
      "source": [
        "# histogram for textual values (note the use of explode when it's a list of string):\n",
        "unprocess_dataset.explode(\"genres\").genres.value_counts().plot.barh(label=\"genres of available shows\")\n",
        "unprocess_dataset[unprocess_dataset.Action == \"play\"].explode(\"genres\").genres.value_counts().plot.barh(color=\"red\", label=\"genres of shows watched\")\n",
        "plt.legend()\n",
        "plt.title(\"Distribution of shows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KgdFm4jKquJ"
      },
      "outputs": [],
      "source": [
        "# correlation between values:\n",
        "unprocess_dataset[[\"imdb_score\", \"tmdb_score\"]].corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3brn38cKquJ"
      },
      "source": [
        "## What's our score here ?\n",
        "\n",
        "- What's your idea ?\n",
        "\n",
        "We base our scoring on the past interactions of the users with the movies when they typed a query.\n",
        "- We consider that if the user watched the movie, it was highly relevant\n",
        "- We consider that if the user added it to it's watchlist, it was relevant, but not the right mood at that time\n",
        "- We consider that if the user clicked on a movie, it showed some interest but it wasn't that relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv-othuzKquJ"
      },
      "outputs": [],
      "source": [
        "from my_first_ltr.utils import load_dataset\n",
        "\n",
        "dataset = load_dataset()\n",
        "dataset.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zJtNJv6KquJ"
      },
      "source": [
        "We could also negative examples. It would help to improve the model by providing contrast to positive examples. Using a conversion ratio in the score, instead of a simple sum, helps the model better capture the relative importance of examples, leading to more balanced and accurate rankings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvOhFuAVKquJ"
      },
      "source": [
        "We split data into training and testing sets to train the model on one portion of the data and evaluate its performance on unseen data, ensuring it generalizes well to new inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hivs_dRAKquK"
      },
      "outputs": [],
      "source": [
        "from my_first_ltr.train_utils import get_categories\n",
        "import pandas as pd\n",
        "from catboost import Pool\n",
        "\n",
        "def dataset_split(\n",
        "    dataset: pd.DataFrame,\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the dataset into training, testing, and validation sets based on queries.\n",
        "\n",
        "    Steps to implement:\n",
        "    1. Group the dataset by the \"normalized_query\"\n",
        "    2. Split the grouped data into two sets: 95% for training and testing, 5% for validation.\n",
        "    3. Further split the 95% dataset into 70% for training and 30% for testing.\n",
        "    4. Explode the grouped data back into individual rows for each query.\n",
        "    5. Return the Datasets\n",
        "    \"\"\"\n",
        "    # FIXME: Complete the function here\n",
        "    pass\n",
        "\n",
        "def build_pool(dataset: pd.DataFrame, name: str) -> Pool:\n",
        "    \"\"\"\n",
        "    Given a dataset and its name, build a CatBoost Pool object.\n",
        "\n",
        "    Steps to implement:\n",
        "    1. Sort the dataset by the \"normalized_query\" column.\n",
        "    2. Prepare the input features:\n",
        "       - Exclude columns that are not input features (e.g., \"normalized_query\", \"id\", \"score\").\n",
        "    3. Extract:\n",
        "       - Features\n",
        "       - Target column (\"score\") as labels.\n",
        "    4. To identify the categorical features in the dataset ou can use the function `get_categories(dataset: pd.DataFrame)`.\n",
        "    5. Construct and return the `Pool` (Group identifiers from the \"normalized_query\" column with the parameter `group_id`).\n",
        "    \"\"\"\n",
        "    # FIXME: Complete the function here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "spkLWqpqKquK"
      },
      "outputs": [],
      "source": [
        "#@title Proposed solution\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_categories(dataset: pd.DataFrame) -> list[str]:\n",
        "    categories = dataset.select_dtypes([\"object\", \"category\"]).columns.to_list()\n",
        "    return categories\n",
        "\n",
        "\n",
        "def _group_per_query(dataset: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Define all features we need to keep for the pool\n",
        "    pool_features = list(set(dataset.columns) - set([\"normalized_query\"]))\n",
        "\n",
        "    # group per query and map the records data\n",
        "    #   query                                                      hits\n",
        "    # 0     a  [{'brand': '1', 'price': 1}, {'brand': '2', 'price': 2}]\n",
        "    grouped_per_query = (\n",
        "        dataset.groupby(\n",
        "            [\"normalized_query\"], sort=False, group_keys=True, as_index=True\n",
        "        )[pool_features]\n",
        "        .apply(lambda x: x.to_dict(\"records\"))\n",
        "        .reset_index()\n",
        "        .rename(\n",
        "            columns={0: \"data\"}\n",
        "        )  # FIXME I think drop=True) in reset index does the same\n",
        "    )\n",
        "\n",
        "    return grouped_per_query\n",
        "\n",
        "\n",
        "def _explode_per_query(grouped_dataset: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Inverse operation of group per query\"\"\"\n",
        "\n",
        "    # Recreate one row per query / data\n",
        "    dataset = grouped_dataset.explode(\"data\")\n",
        "\n",
        "    # remap the data content to columns\n",
        "    dataset = pd.concat([dataset, dataset[\"data\"].apply(pd.Series)], axis=1).drop(\n",
        "        columns=[\"data\"]\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def dataset_split(\n",
        "    dataset: pd.DataFrame,\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    grouped_per_query = _group_per_query(dataset)\n",
        "\n",
        "    train_test_queries_df, val_queries_df = train_test_split(\n",
        "        grouped_per_query, test_size=0.05, shuffle=False\n",
        "    )\n",
        "    train_queries_df, test_queries_df = train_test_split(\n",
        "        train_test_queries_df, test_size=0.3, shuffle=False\n",
        "    )\n",
        "\n",
        "    train_df = _explode_per_query(train_queries_df)\n",
        "    test_df = _explode_per_query(test_queries_df)\n",
        "    val_df = _explode_per_query(val_queries_df)\n",
        "\n",
        "    print(\"After split: train dataset\", train_df.shape)\n",
        "    print(\"After split: test dataset\", test_df.shape)\n",
        "    print(\"After split: validation dataset\", val_df.shape)\n",
        "\n",
        "    return train_df, test_df, val_df\n",
        "\n",
        "\n",
        "def validate_not_empty(df: pd.DataFrame, name: str) -> None:\n",
        "    \"\"\"Ensure a dataframe is not empty. Raise an error if it's the case.\"\"\"\n",
        "    if df.shape[0] == 0:\n",
        "        raise ValueError(f\"Dataset {name} should not be empty, shape={df.shape}\")\n",
        "\n",
        "\n",
        "def keep_input_features(dataset: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Filter out target, queries and item id.\"\"\"\n",
        "    return dataset.drop(columns=[\"normalized_query\", \"id\", \"score\"])\n",
        "\n",
        "\n",
        "def build_pool(dataset: pd.DataFrame, name: str) -> Pool:\n",
        "    \"\"\"Given the config and the dataset build the Catboost pool\"\"\"\n",
        "    validate_not_empty(dataset, name)\n",
        "    dataset = dataset.sort_values(by=[\"normalized_query\"])\n",
        "\n",
        "    X = keep_input_features(dataset)\n",
        "    y = dataset[\"score\"]\n",
        "    queries = dataset[\"normalized_query\"].values\n",
        "\n",
        "    categories = get_categories(X)\n",
        "\n",
        "    pool = Pool(\n",
        "        data=X,\n",
        "        label=y,\n",
        "        group_id=queries,\n",
        "        cat_features=categories,\n",
        "        thread_count=1,\n",
        "    )\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFpNMW7AKquK"
      },
      "outputs": [],
      "source": [
        "from my_first_ltr.train_utils import build_pool, dataset_split\n",
        "\n",
        "\n",
        "train_df, test_df, val_df = dataset_split(dataset)\n",
        "\n",
        "train_pool = build_pool(train_df, \"train\")\n",
        "test_pool = build_pool(test_df, \"test\")\n",
        "val_pool = build_pool(val_df, \"validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIlgdp_nKquK"
      },
      "source": [
        "# Then comes a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU-RTE8iKquK"
      },
      "source": [
        "## Pointwise: RMSE\n",
        "\n",
        "A pointwise learning-to-rank (LTR) approach using Root Mean Square Error (RMSE) is a method where the ranking problem is treated as a regression problem. The model is trained to predict a relevance score as close as possible to the ground truth relevance score for each individual item.\n",
        "\n",
        "**It ignores the relationships between items within a list, focusing only on the accuracy of individual predictions. Thus it's name, pointwise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJn6-fotKquK"
      },
      "source": [
        "### Let's go to practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHWrunzfKquK"
      },
      "source": [
        "We initializes a CatBoostRanker, a gradient-boosting model  using the following parameters:\n",
        "\n",
        "- `loss_function=\"RMSE\"`: Optimize based on `RMSE` which measures the average squared difference between predicted and true ranks.\n",
        "- `learning_rate=0.15`: Determines how much the model's parameters are updated in response to the calculated error after each iteration. A smaller value leads to slower, more stable learning, while a larger value speeds learning but risks overshooting the optimal solution.\n",
        "- `thread_count=1`: Uses a single CPU thread for training.\n",
        "- `iterations=500`: Runs 500 iterations of boosting (adding weak learners to improve predictions).\n",
        "- `random_seed=0`: Ensures reproducible results by fixing randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "623TT5_kKquK"
      },
      "outputs": [],
      "source": [
        "\n",
        "from catboost import CatBoostRanker\n",
        "\n",
        "model = CatBoostRanker(loss_function=\"RMSE\", depth=6, learning_rate=0.15, thread_count=1, iterations=500, random_seed=0)\n",
        "\n",
        "model.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGEhGa7zKquK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "val_df[\"pred_score\"] = model.predict(val_pool)\n",
        "\n",
        "results = val_df[[\"score\", \"pred_score\"]]\n",
        "\n",
        "print(\"Predictions vs Actuals:\")\n",
        "print(results.head())\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(results[\"score\"], results[\"pred_score\"]))\n",
        "mae = mean_absolute_error(results[\"score\"], results[\"pred_score\"])\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtyNgltuKquL"
      },
      "source": [
        "## Pairwise: PairLogit\n",
        "\n",
        "This approach focuses on learning the relative preference between pairs of items. The model is trained to predict which of the two items in a pair should be ranked higher, based on pairwise comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id5EqiE_KquL"
      },
      "outputs": [],
      "source": [
        "# FIXME: setup the ranker for some pairwise ranking\n",
        "model_pairwise = CatBoostRanker(loss_function=\"PairLogit\", thread_count=1, random_seed=0)\n",
        "model_pairwise.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEh5lb7oKquL"
      },
      "source": [
        "## Listwise: YetiRank\n",
        "\n",
        "YetiRank optimizes a smooth approximation of an IR (Information Retrieval) metric, such as NDCG (Normalized Discounted Cumulative Gain). The use of a listwise approach means that the model learns directly to improve the ranking quality of the entire list rather than individual scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cijsu5-vKquL"
      },
      "source": [
        "### Wait, NDC what ?\n",
        "\n",
        "Normalized Discounted Cumulative Gain (NDCG) is a metric used to evaluate the quality of a ranked list of items. It measures how well the ranking of retrieved items matches the ideal ranking based on relevance, emphasizing the importance of placing highly relevant items near the top of the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcBgGzXWKquL"
      },
      "outputs": [],
      "source": [
        "# documents ordered according to their relevance scores\n",
        "from numpy import asarray\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "\n",
        "true_relevance = asarray([list(reversed(range(21)))])\n",
        "print(true_relevance)\n",
        "print(\"Perfect:\", ndcg_score(true_relevance, true_relevance))\n",
        "\n",
        "pred_relevance = asarray([[19, 20] + list(reversed(range(19)))])\n",
        "print(pred_relevance)\n",
        "print(\"Two items swapping places:\", ndcg_score(true_relevance, pred_relevance))\n",
        "\n",
        "pred_relevance = asarray([[15, 19, 18, 17, 16, 20] + list(reversed(range(15)))])\n",
        "print(pred_relevance)\n",
        "print(\"Two items swapping places further down:\", ndcg_score(true_relevance, pred_relevance))\n",
        "\n",
        "pred_relevance = asarray([list(range(21))])\n",
        "print(pred_relevance)\n",
        "print(\"Let's reverse everything\", ndcg_score(true_relevance, pred_relevance))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtx3eKWSKquL"
      },
      "source": [
        "### Back to our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykNlUJlCKquL"
      },
      "outputs": [],
      "source": [
        "# FIXME: setup the ranker for some listwise ranking\n",
        "# As you can see, we have to specify the end metric required to optimize here, (eg: CTR, CVR, NDCG) as we are not basing\n",
        "# the optimization of the scores difference to prediction.\n",
        "model_listwise = CatBoostRanker(loss_function=\"YetiRank\", thread_count=1, random_seed=0, custom_metric=[\"NDCG:top=-1;type=Base;denominator=LogPosition;hints=skip_train~false\"])\n",
        "model_listwise.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKNQVCW2KquL"
      },
      "source": [
        "## Model's leaderboard\n",
        "\n",
        "Create a model's leaderboard add add your iteration to compare to our baselines!\n",
        "\n",
        "Little tip to get you started, you can use catboost `get_eval` method to quickly retrieve a metric for a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n-v0B7tKquL"
      },
      "outputs": [],
      "source": [
        "from my_first_ltr.data_visualisation import RMSE, NDCG\n",
        "\n",
        "# FIXME: try out the eval_metrics\n",
        "your_metric = ...\n",
        "model.eval_metrics(train_pool, your_metric, ntree_start=model.tree_count_ - 1)\n",
        "\n",
        "# FIXME: Compare the different models.\n",
        "models = {\"RMSE\": model, \"BestModelInTheWorld\": ...}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xxYNFe2TKquL"
      },
      "outputs": [],
      "source": [
        "#@title Solution to retrieve metrics for multiple models\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "models = {\"RMSE\": model, \"PairLogit\": model_pairwise, \"YetiRank\": model_listwise}\n",
        "metrics = []\n",
        "\n",
        "for k, m in models.items():\n",
        "    metrics_dict = dict()\n",
        "    metrics_dict[\"model_name\"] = k\n",
        "    metrics_dict['train_NDCG@20'] = m.eval_metrics(train_pool,\n",
        "                                                      NDCG,\n",
        "                                                      ntree_start=m.tree_count_ - 1)['NDCG:type=Base'][0]\n",
        "\n",
        "    metrics_dict['test_NDCG@20'] = m.eval_metrics(test_pool,\n",
        "                                                     NDCG,\n",
        "                                                     ntree_start=m.tree_count_ - 1)['NDCG:type=Base'][0]\n",
        "\n",
        "    metrics_dict['val_NDCG@20'] = m.eval_metrics(val_pool,\n",
        "                                                NDCG,\n",
        "                                                ntree_start=m.tree_count_ - 1)['NDCG:type=Base'][0]\n",
        "    metrics.append(metrics_dict)\n",
        "\n",
        "\n",
        "metrics_df = pd.DataFrame.from_records(metrics)\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTRuEUP3KquM"
      },
      "source": [
        "## Feature importance: SHAP values\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) values are a concept to explain predictions made by machine learning models. They are based on Shapley values, from game theory. They allow to map the gain brough by each player.\n",
        "\n",
        "One way to picture it, is, what would we loose if we'd remove a player (feature) from a poker game (from the input).\n",
        "\n",
        "Shap values are feature dependent, meaning, the gain of one player depends of the other players' hands. A same feature value (eg, category=\"western\") can have a different shap values depending on other features (eg,  age_restriction=\"PG13\" or age_restriction=\"PG7\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-hkPCDNKquM"
      },
      "outputs": [],
      "source": [
        "from src.my_first_ltr.data_visualisation import cross_dataset_plot_shap_values\n",
        "\n",
        "# TODO: Have a look at the different feature and the important ones for the model. Does it make sense to you ?\n",
        "# Here is a sample for one of the models to get you started.\n",
        "cross_dataset_plot_shap_values(m, {\"train\": train_pool, \"test\": test_pool, \"val\": val_pool}, top_n=20)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}