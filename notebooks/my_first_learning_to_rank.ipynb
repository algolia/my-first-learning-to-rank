{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"my_first_ltr\"): # type: ignore\n",
    "    %pip install -qqq git+https://github.com/algolia/my-first-learning-to-rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Learning To Rank ?\n",
    "\n",
    "Ranking is the process of organizing or arranging items in order based on their importance, quality, or performance. It is widely used in various applications, such as search engines, where results are ranked to show the most relevant pages first, or in recommender systems, which rank products, movies, or other items to suggest the most suitable options to users.\n",
    "\n",
    "## Why rank ? And what's ranking here ?\n",
    "In the context of a search experience ranking helps prioritize the most relevant results, improving the user experience by making it easier to find what they need quickly. We will try to define an optimal order for the results for each query. What we define by query here is the word or phrase a user types to find information.\n",
    "\n",
    "## What's ranking model ?\n",
    "\n",
    "it's a function that maps an item to it's relevance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It all starts with data\n",
    "\n",
    "- a user search history on a streaming platform\n",
    "- a subset of imdb dataset\n",
    "\n",
    "At home, you can try with your search history if you'd like!\n",
    "`# TODO: if so time, clean up the data prep script in a notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all the nasty pre-processing and cleaning for you - so you can just have fun! \n",
    "The main steps we did for preprocessing are:\n",
    "- One-hot encoding: turn multi-categorical features into a list of binary features\n",
    "- Create a textual relevance signal (using [OkapiBM25](https://en.wikipedia.org/wiki/Okapi_BM25))\n",
    "- Compute the relevance score of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are our features ?\n",
    "What is a feature in our context? A feature in machine learning is a piece of information or characteristic that helps the model make predictions or decisions. Itâ€™s an input the model uses to learn patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.utils import load_raw_dataset\n",
    "\n",
    "# FIXME: remove for dataset\n",
    "unprocess_dataset = load_raw_dataset()\n",
    "unprocess_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead! Explore the dataset to get familiar with it a bit, here are a few examples for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram for numerical values:\n",
    "unprocess_dataset.imdb_score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram for textual values (note the use of explode when it's a list of string):\n",
    "unprocess_dataset.explode(\"genres\").genres.value_counts().plot.barh()\n",
    "unprocess_dataset[unprocess_dataset.Action == \"play\"].explode(\"genres\").genres.value_counts().plot.barh(color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between values:\n",
    "unprocess_dataset[[\"imdb_score\", \"tmdb_score\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's our score here ?\n",
    "\n",
    "- What's your idea ?\n",
    "\n",
    "We base our scoring on the past interactions of the users with the movies when they typed a query. \n",
    "- We consider that if the user watched the movie, it was highly relevant\n",
    "- We consider that if the user added it to it's watchlist, it was relevant, but not the right mood at that time\n",
    "- We consider that if the user clicked on a movie, it showed some interest but it wasn't that relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.utils import load_dataset\n",
    "\n",
    "# TODO: check if once repo is public that loading works\n",
    "dataset = load_dataset(local=True)\n",
    "dataset.head(5)\n",
    "\n",
    "\n",
    "# TODO: stuff we've learn and you should not waist time on\n",
    "\n",
    "\n",
    "# TODO: show a specific query with results examples ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also negative examples. It would help to improve the model by providing contrast to positive examples. Using a conversion ratio in the score, instead of a simple sum, helps the model better capture the relative importance of examples, leading to more balanced and accurate rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then comes a model\n",
    "\n",
    "## PointWise\n",
    "## PairWise\n",
    "## ListWise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and testing sets to train the model on one portion of the data and evaluate its performance on unseen data, ensuring it generalizes well to new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.train_utils import build_pool, dataset_split\n",
    "\n",
    "\n",
    "train_df, test_df, val_df = dataset_split(dataset)\n",
    "\n",
    "train_pool = build_pool(train_df, \"train\")\n",
    "test_pool = build_pool(test_df, \"test\")\n",
    "val_pool = build_pool(val_df, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE pointwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initializes a CatBoostRanker, a gradient-boosting model  using the following parameters:\n",
    "\n",
    "- `loss_function=\"RMSE\"`: Optimize based on `RMSE` which measures the average squared difference between predicted and true ranks.\n",
    "- `learning_rate=0.15`: Determines how much the model's parameters are updated in response to the calculated error after each iteration. A smaller value leads to slower, more stable learning, while a larger value speeds learning but risks overshooting the optimal solution.\n",
    "- `thread_count=1`: Uses a single CPU thread for training.\n",
    "- `iterations=500`: Runs 500 iterations of boosting (adding weak learners to improve predictions).\n",
    "- `random_seed=0`: Ensures reproducible results by fixing randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostRanker\n",
    "\n",
    "model = CatBoostRanker(loss_function=\"RMSE\", depth=6, learning_rate=0.15, thread_count=1, iterations=500, random_seed=0)\n",
    "\n",
    "\n",
    "model.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "val_df[\"pred_score\"] = model.predict(val_pool)\n",
    "\n",
    "\n",
    "results = val_df[[\"score\", \"pred_score\"]]\n",
    "\n",
    "print(\"Predictions vs Actuals:\")\n",
    "print(results.head())\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(results[\"score\"], results[\"pred_score\"]))\n",
    "mae = mean_absolute_error(results[\"score\"], results[\"pred_score\"])\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: setup the ranker for some pairwise ranking\n",
    "model_pairwise = CatBoostRanker(loss_function=\"PairLogit\", thread_count=1)\n",
    "model_pairwise.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: setup the ranker for some listwise ranking\n",
    "# As you can see, we have to specify the end metric required to optimize here, (eg: CTR, CVR, NDCG) as we are not basing\n",
    "# the optimization of the scores difference to prediction.\n",
    "model_listwise = CatBoostRanker(loss_function=\"YetiRank\", thread_count=1, custom_metric=[\"NDCG:top=-1;type=Base;denominator=LogPosition;hints=skip_train~false\"])\n",
    "model_listwise.fit(train_pool, eval_set=test_pool, plot=True, metric_period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_first_ltr.data_visualisation import RMSE, MAP, MAE, NDCG_20\n",
    "\n",
    "# FIXME: Compare the different models. Little tip to get you started, you can use catboost `get_eval` method\n",
    "\n",
    "your_metric = ...\n",
    "model.eval_metrics(train_pool, your_metric, ntree_start=model.tree_count_ - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution to retrieve metrics for multiple models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "models = {\"RMSE\": model, \"PairLogit\": model_pairwise, \"YetiRank\": model_listwise}\n",
    "metrics = []\n",
    "\n",
    "for k, m in models.items():\n",
    "    metrics_dict = dict()\n",
    "    metrics_dict[\"model_name\"] = k\n",
    "    metrics_dict['train_NDCG@20'] = m.eval_metrics(train_pool,\n",
    "                                                      'NDCG:top=20;type=Base;denominator=LogPosition',\n",
    "                                                      ntree_start=m.tree_count_ - 1)['NDCG:top=20;type=Base'][0]\n",
    "\n",
    "    metrics_dict['test_NDCG@20'] = m.eval_metrics(test_pool,\n",
    "                                                     'NDCG:top=20;type=Base;denominator=LogPosition',\n",
    "                                                     ntree_start=m.tree_count_ - 1)['NDCG:top=20;type=Base'][0]\n",
    "\n",
    "    metrics_dict['val_NDCG@20'] = m.eval_metrics(val_pool,\n",
    "                                                'NDCG:top=20;type=Base;denominator=LogPosition',\n",
    "                                                ntree_start=m.tree_count_ - 1)['NDCG:top=20;type=Base'][0]\n",
    "    metrics.append(metrics_dict)\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame.from_records(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoost, Pool\n",
    "from my_first_ltr.train_utils import keep_input_features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_dataset_shape_cascade( m: CatBoost, X: pd.DataFrame, pools: dict[str, Pool]) -> None:\n",
    "    \"\"\"Shap values average per feature accross all dataset.\"\"\"\n",
    "    df_feature_importance = pd.DataFrame(\n",
    "        data={k: m.get_feature_importance(pool) for k, pool in pools.items()}, index=X.columns\n",
    "    )\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    df_feature_importance.plot.barh(figsize=(10, 12))\n",
    "    plt.title(\"cross dataset - importance of each feature\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plt.savefig(os.path.join(path, f\"features_importances_{config.impl_name}.png\"), format=\"png\", bbox_inches=\"tight\")\n",
    "\n",
    "cross_dataset_shape_cascade(model, keep_input_features(train_df), {\"train\": train_pool, \"test\": test_pool, \"val\": val_pool})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
